README - RAG tests with Mistral API and ChromaDB
====================================================================

Overview
--------
This project implements a local document indexing pipeline combined with a retrieval-augmented generation (RAG) system using embeddings and the Mistral language model API. It indexes various document formats into a vector database (ChromaDB) and enables querying with contextualized answers generated by Mistral.

Components
----------
1. **main_index.py**  
   - Indexes documents from a configured source folder into a local persistent ChromaDB database.  
   - Supports `.txt`, `.pdf`, `.docx`, `.csv`, `.xls`, and `.xlsx` formats.  
   - Text is chunked into overlapping segments, embedded with a local SentenceTransformer model on GPU, and stored with metadata.  
   - Maintains progress in a file to avoid reprocessing.

2. **config.py**  
   - Contains project constants and folder/file paths.  
   - Important constants: `CHROMA_PATH`, `SOURCE_FOLDER`, `CHUNK_SIZE`, `CHUNK_OVERLAP`, `BATCH_SIZE`, and `PROGRESS_FILE`.  
   - Note: `SOURCE_FOLDER` must be assigned a valid Path object pointing to the documents directory.

3. **main_rag.py**  
   - Simple example script demonstrating querying the ChromaDB using an embedding generated via Mistral embedding API.  
   - Retrieves top 5 relevant documents.  
   - Sends a prompt combining context and question to the Mistral large model API for response generation.

5. **wout_rag.py**  
   - Minimal example of a standalone Mistral chat completion API call without any RAG.  
   - Sends a question prompt and prints the model's answer.

6. **[BUILDING] Another RAG Pipeline using langchain**
    - Not used yet because way more time consuming


Setup and Requirements
----------------------
- Python 3.8+ environment recommended.  
- Install dependencies: `chromadb`, `sentence_transformers`, `PyPDF2`, `docx` (python-docx), `pandas`, `requests`, `tqdm`.  
- A CUDA-capable GPU and drivers to run SentenceTransformer models on GPU.  
- Set environment variable `MISTRAL_API_KEY` with your Mistral API key for API calls.  
- Configure `SOURCE_FOLDER` in `config.py` as a valid `Path` to your documents folder before running `main.py`.  
- Ensure `CHROMA_PATH` directory is writable for ChromaDB persistence.

Usage
-----
1. **Index documents:**  
   Run `python main_index.py` to recursively process and index supported documents into ChromaDB.

2. **Query with context:**  
   - Use `langchain_index.py` to run a sample question embedding and Mistral API call with retrieved documents.  

3. **Simple Mistral chat example:**  
   Run `wout_rag.py` to send a direct question to the Mistral chat API without document context.

Notes
-----
- Ensure your documents are clean and correctly formatted for extraction to improve embedding quality.  
- Embedding and API model names are configurable; you may choose other available Mistral models or SentenceTransformer variants.  
- Progress tracking avoids re-indexing files, allowing safe interruption and resume.  
- Error handling for extraction and API calls is included but can be expanded for production use.

Security
--------
- Do not hardcode your API keys in code for production. Use environment variables or secure vaults.  
- Validate and sanitize inputs if integrating into larger systems.

License
-------
This code is provided as-is for educational and research purposes.

Contact
-------
For questions or improvements, modify as needed or reach out to the original author.

